{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130bf866-9e5e-455e-be15-52adac069107",
   "metadata": {},
   "source": [
    "## Drop column from Delta Lake table\n",
    "\n",
    "This notebook demonstrates how to drop a column of a Delta Lake table.\n",
    "\n",
    "It demonstrates how the column mapping functionality that was added in Delta 1.2 makes this operation a lot more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0514c20-8f06-48cf-8afc-8ccb633db556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b4c1ed-849b-4da2-aeec-b691fa623ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37dbca2-be99-4907-bd29-3b987e5f3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-583f79c3-ad4d-46a1-aaab-e14079a498ae;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 331ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-583f79c3-ad4d-46a1-aaab-e14079a498ae\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/13 10:31:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c5640-f0cb-4464-b5c0-f1c2fbfa9ac4",
   "metadata": {},
   "source": [
    "## Create Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a4a182-5164-4b04-b03f-fd839359ef40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists `my_cool_table`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97f9f95-d7e1-4dd1-af28-9f029012233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns = [\"language\", \"num_speakers\"]\n",
    "data = [(\"English\", \"1.5\"), (\"Mandarin\", \"1.1\"), (\"Hindi\", \"0.6\")]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = rdd.toDF(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618bc645-d64d-41ee-ae12-7aec8c50f382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|language|num_speakers|\n",
      "+--------+------------+\n",
      "| English|         1.5|\n",
      "|Mandarin|         1.1|\n",
      "|   Hindi|         0.6|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec3d534-a899-4f1f-a769-cbd843a1a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/13 10:32:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.sql.delta.DeltaAnalysisException: Cannot create table ('`default`.`my_cool_table`'). The associated location ('file:/Users/matthew.powers/Documents/code/my_apps/delta-examples/notebooks/pyspark/spark-warehouse/my_cool_table') is not empty but it's not a Delta table\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.createTableWithNonEmptyLocation(DeltaErrors.scala:2226)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.createTableWithNonEmptyLocation$(DeltaErrors.scala:2225)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.createTableWithNonEmptyLocation(DeltaErrors.scala:2293)\n",
      "\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.assertPathEmpty(CreateDeltaTableCommand.scala:248)\n",
      "\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:120)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)\n",
      "\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:49)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
      "\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:49)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)\n",
      "\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:49)\n",
      "\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:109)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:162)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:432)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:507)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:636)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Cannot create table ('`default`.`my_cool_table`'). The associated location ('file:/Users/matthew.powers/Documents/code/my_apps/delta-examples/notebooks/pyspark/spark-warehouse/my_cool_table') is not empty but it's not a Delta table",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault.my_cool_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1041\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot create table ('`default`.`my_cool_table`'). The associated location ('file:/Users/matthew.powers/Documents/code/my_apps/delta-examples/notebooks/pyspark/spark-warehouse/my_cool_table') is not empty but it's not a Delta table"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").saveAsTable(\"default.my_cool_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625373ef-f44a-43a5-a526-c3e6d3449048",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from `my_cool_table`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84e283-6335-4308-a0da-616b0955a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree ./spark-warehouse/my_cool_table/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd03224-9428-4ed8-8b98-f9f5af0025b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from `my_cool_table`\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0108d6-b50b-41e6-8d31-32f1cdbf5df4",
   "metadata": {},
   "source": [
    "## Drop column from Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee490d5-6680-434b-9177-2fc20cbc0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"ALTER TABLE `my_cool_table` SET TBLPROPERTIES (\n",
    "   'delta.columnMapping.mode' = 'name',\n",
    "   'delta.minReaderVersion' = '2',\n",
    "   'delta.minWriterVersion' = '5')\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acf7ce-c549-4c7c-a1c9-f5a050bb11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"alter table `my_cool_table` drop column language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1d76f-c0d0-466a-895b-b845c0c46ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from `my_cool_table`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbd8b4-27e4-4859-8d08-b867fcb539d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree ./spark-warehouse/my_cool_table/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7d4a7-b689-48e1-86af-bac8410cf160",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from `my_cool_table`\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83633ede-87bc-42e4-921b-8ce9e1c8a1c1",
   "metadata": {},
   "source": [
    "## Drop column from Delta Lake pre Delta 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bad534-9b3f-4aa8-bf63-88bb83c9775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists `another_cool_table`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca18ced-a8e5-4a17-8b91-b2fa3bba3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\", \"num_speakers\"]\n",
    "data = [(\"Spanish\", \"0.5\"), (\"French\", \"0.3\"), (\"Arabic\", \"0.3\")]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = rdd.toDF(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96069c35-c824-4dd2-8047-df814bb2ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").saveAsTable(\"default.another_cool_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e0266-ac49-4282-842f-22727101da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from another_cool_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c3140-84d6-489d-bd03-9b6054838566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94238756-88f9-4fc6-b632-492e36b188e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -l ./spark-warehouse/another_cool_table/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3884d-e699-4a6d-a30b-96fbe21612a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"num_speakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad1256-2c18-4675-aa4b-a2f9963867b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a301899-6612-427d-a883-621c88b3ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"OVERWRITE\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(\"default.another_cool_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab98d0-3d3c-40b5-87a6-b04811a605d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from another_cool_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde5353-a4a5-435a-bcf6-8324ac8786e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -l ./spark-warehouse/another_cool_table/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38582a-8422-4a7d-9c45-add6bd67b693",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c258a-a636-43a3-91a7-68dc1ce98e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists `my_cool_table`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea08213-ae7f-4040-a442-99e81069cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists `another_cool_table`\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-330-delta-210] *",
   "language": "python",
   "name": "conda-env-pyspark-330-delta-210-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
